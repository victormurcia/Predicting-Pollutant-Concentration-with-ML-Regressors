{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d908ac",
   "metadata": {},
   "source": [
    "# Exploring the Air Quality Dataset\n",
    "Air pollution is a major concern for the modern world. \n",
    "\n",
    "Prolonged exposure to low quality air resulting from  a variety of sources like vehicle emissions, fumes from chemical production, mold spores, and wildfires can lead to several adverse health effects. Some of the health effects from exposure to poor air quality are irritation of the eyes, nose and throat, shortness of breath, afflictions on the cardiovascular system and even more serious complications after long periods of time. \n",
    "\n",
    "<img src = \"pollution air.png\">\n",
    "\n",
    "Apart from the impact that low air quality has on human health, the environmental effects of poor air quality are just as severe. Poor air quality can lead to reduced crop yields as well as increased susceptibility of plants to pests and disease amongst other things.\n",
    "\n",
    "As such keeping a close eye on air quality levels is of import to health and the environment. \n",
    "\n",
    "In this project I'll be exploring the AirQuality Dataset in order to train a machine learning (ML) model capable of predicting predicting the temperature based on the concentrations of a variety of pollutants like metal oxides and hydrocarbons. This will also give me chance to work on time series data. \n",
    "\n",
    "The data was obtained from the UCI Machine Learning Repository here <a href = \"https://archive.ics.uci.edu/ml/datasets/Air+Quality\">https://archive.ics.uci.edu/ml/datasets/Air+Quality</a>. This dataset covers sensor collection data on a variety of pollutants spanning from March 2004 to February 2005 (1 year) \n",
    "\n",
    "## Modules Used in this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import *\n",
    "\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button\n",
    "import seaborn as sns\n",
    "\n",
    "#For dealing with time string types\n",
    "import datetime\n",
    "\n",
    "#For Exploratory Data Analysis (EDA)\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "#For building ML model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Different Regressors for ML model\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel , RBF\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#For model evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "#For parameter grid search optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#For saving model\n",
    "import pickle\n",
    "\n",
    "#For q-q plot\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe6c8d",
   "metadata": {},
   "source": [
    "## Loading the Air Quality Data\n",
    "I'll start by loading the data. The data was in .csv format which can be readily loaded into a pandas dataframe using the <code>read_csv</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq = pd.read_csv(\"AirQualityUCI.csv\", sep = \";\", decimal = \",\")        \n",
    "aq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa4b9b",
   "metadata": {},
   "source": [
    "Ok. The data has 9357 observations and 17 different columns. However, there are two columns (labeled as Unnamed 15 and 16) that have garbage on them that I'll have to remove. different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq.drop(['Unnamed: 15','Unnamed: 16'], axis=1, inplace=True, errors = 'ignore') \n",
    "aq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a86f8f",
   "metadata": {},
   "source": [
    "Good. Those columns are gone. Next, I'll check for null values and ensure that all the data is properly formatted. \n",
    "## Removing Null/Faulty Sensor Readings\n",
    "From the dataset documentation,it is known that faulty or missing sensor readings were filled with the value '-200'. Therefore, I'll handle those instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing bad sensor readings designated by an entry of -200 with NaN\n",
    "aq.replace(to_replace = -200, value = np.nan, inplace = True)\n",
    "aq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbee88",
   "metadata": {},
   "source": [
    "Looks like Column 4 labeled as 'NMHC(GT) has a lot of faulty readings relative to all the other sensors. Let me determine what the percent composition is of nulls with respect to the total values. This will help me determine whether to keep that column or drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb69082",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_NaN = []\n",
    "columns = aq.columns\n",
    "for col in columns:\n",
    "    pNaN =  (aq[col].isna().sum()/aq.shape[0]) * 100 #sum NaN instances in each column. Divide by total rows\n",
    "    percent_NaN.append(pNaN)\n",
    "nan_percent_df = pd.DataFrame(percent_NaN,\n",
    "                              index=columns,\n",
    "                              columns=['%_NaN_in_Column']).sort_values('%_NaN_in_Column',ascending = False)\n",
    "nan_percent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91562c15",
   "metadata": {},
   "source": [
    "Yeah... more than 90% of the entries in that columns are null. If I were to use the dropna method on the entire dataset I'd end up losing 90% of our data. Furthermore, we still have a column with information about Benzene (C6H6) concentrations (benzene is a hydrocarbon) through the columns C6H6(GT) and PT08.S2(NMHC), so we haven't completely lost the entirety of the hydrocarbon readings. Instead, I'll opt for dropping this feature from the dataset, dropping the NaN rows and press forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f21ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq.drop('NMHC(GT)', axis=1, inplace=True, errors = 'ignore') \n",
    "aq = aq.dropna()\n",
    "aq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479acf3",
   "metadata": {},
   "source": [
    "Ok, let's check the dataframe info again to see how things are looking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993da144",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73bee48",
   "metadata": {},
   "source": [
    "Hmm the data in the Date and Time columns should be formatted as datetime objects and they are not. I'll need to fix that.\n",
    "## Cleaning Up Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq['DateTime'] =  (aq.Date) + ' ' + (aq.Time)\n",
    "aq.DateTime = aq.DateTime.apply(lambda x: datetime.datetime.strptime(x, '%d/%m/%Y %H.%M.%S'))\n",
    "aq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee791152",
   "metadata": {},
   "source": [
    "I tried several different strategies to try to convert the Time column to an actual time. The method below was the one that gave me the result I wanted. Now, I'll drop the original Date and Time Columns and generate columns for Day, Month, and Time from the DateTime column I just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq['Weekday'] = aq['DateTime'].dt.day_name()\n",
    "aq['Month']   = aq['DateTime'].dt.month_name()\n",
    "aq['Hour']    = aq['DateTime'].dt.hour\n",
    "aq['Date']    = pd.to_datetime(aq['Date'], format='%d/%m/%Y')\n",
    "aq.drop('Time', axis=1, inplace=True, errors = 'ignore') \n",
    "aq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c8434",
   "metadata": {},
   "source": [
    "Almost there. Now I'll just reorder the columns a bit and then I'm good to go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq = aq[['Date','Month', 'Weekday','DateTime', 'Hour', 'CO(GT)','PT08.S1(CO)', 'C6H6(GT)', 'PT08.S2(NMHC)', \n",
    "         'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']]\n",
    "aq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1bd03",
   "metadata": {},
   "source": [
    "Neat! And let's do a final check of formatting before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdede983",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ec604",
   "metadata": {},
   "source": [
    "Perfect! Now all the data is nice and tidy. \n",
    "\n",
    "## Checkpoint for DataFrame Profiling\n",
    "These are the data features that we have at our disposal from this dataset. \n",
    "\n",
    "* 0 Date (DD/MM/YYYY)\n",
    "* 1 Time (HH.MM.SS)\n",
    "* 2 True hourly averaged concentration CO in mg/m^3 (reference analyzer)\n",
    "* 3 PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)\n",
    "* 4 True hourly averaged overall Non-metallic HydroCarbons concentration in microg/m^3 (reference analyzer)\n",
    "* 5 True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)\n",
    "* 6 PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)\n",
    "* 7 True hourly averaged NOx concentration in ppb (reference analyzer)\n",
    "* 8 PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)\n",
    "* 9 True hourly averaged NO2 concentration in microg/m^3 (reference analyzer)\n",
    "* 10 PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)\n",
    "* 11 PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)\n",
    "* 12 Temperature in °C\n",
    "* 13 Relative Humidity (%)\n",
    "* 14 AH Absolute Humidity\n",
    "\n",
    "Next, I'll use pandas profiling to quickly get a report on the data we have. This will give me a quick overview of what variables would be interesting to explore for a model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac03676",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq.reset_index(drop=True, inplace=True) #Drop index prior to generating report\n",
    "report = ProfileReport(aq)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff053eb",
   "metadata": {},
   "source": [
    "There's a few interesting things that report has shown. \n",
    "\n",
    "In particular, it can be seen that all the pollutant concentrations detected by the sensors are highly correlated with one another. Interestingly, for some reason the readings from the sensor 'PT08.S3(NOx)' are anticorrelated with one another. This would suggest that the levels of Nitrous Oxide(s) are decreasing with increasing levels of the other pollutants. Why would this be? Is this a real effect?\n",
    "\n",
    "Other things that are noteworthy are the distribution shapes for each variable. Most of them appear to have approximately unimodal distributions (that also show right-skewedness). Some of the exceptions are PT08.S4(NO2) are AH where there is fairly clear bimodal behavior occurring. This suggests that there is further cleaning up that I need to do since skewedness in a distribution is related to the presence of outliers. \n",
    "## Removing Outliers From Dataframe\n",
    "I'll deal with the outliers by using the interquartile range (IQR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Outliers with the Interquartile Range Method (IQR)\n",
    "Q1 = aq.quantile(0.25) #first 25% of the data\n",
    "Q3 = aq.quantile(0.75) #first 75% of the data\n",
    "IQR = Q3 - Q1 #IQR = InterQuartile Range\n",
    "\n",
    "scale = 1.4 #May need to play with this value to modify outlier detection sensitivity if need be\n",
    "lower_lim = Q1 - scale*IQR\n",
    "upper_lim = Q3 + scale*IQR\n",
    "\n",
    "cols = aq.columns[5:] # Look for oulierts in columns starting from CO(GT)\n",
    "\n",
    "#Mask a masking condition that removes rows that have values above/below IQR limits\n",
    "condition = ~((aq[cols] < lower_lim) | (aq[cols] > upper_lim)).any(axis=1)\n",
    "\n",
    "#Generate new dataframe that has had its outliers removed\n",
    "aq_filtered = aq[condition]\n",
    "aq_filtered#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abcbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_filitered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3718996",
   "metadata": {},
   "source": [
    "Cool. Now I've removed outliers and done my due dilligence to ensure the data is clean and ready to be processed. \n",
    "\n",
    "Let me generate another profile on the cleaned up data to ensure that we've properly dealt with skewedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31523a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_filtered.reset_index(drop=True, inplace=True) #Drop index prior to generating report\n",
    "report = ProfileReport(aq_filtered)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a29681",
   "metadata": {},
   "source": [
    "Hmmm, I've played around with different outlier sensitivities using the IQR and CO(GT), NOx(GT) and C6H6 columns still have several outliers causing skewedness. \n",
    "\n",
    "As I mentioned earlier, C6H6 is a hydrocarbon and we have another sensor that is detecting nonmetallic hydrocarbons (NMHC). Therefore, the C6H6 column is sorta redundant and we can drop it without losing too much information. \n",
    "\n",
    "With NOx, we still have a sensor in the dataframe responsible for detecting nitrous oxides. This means that the NOx column is similarly redundant.\n",
    "\n",
    "With CO(GT), we still have a sensor in the dataframe responsible for specifically detecting CO. This means that the CO(GT) column is also redundant.\n",
    "\n",
    "Using that same logic, I'll drop the NO2(GT) column as well. \n",
    "\n",
    "As such, I'll remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_filtered.drop(['CO(GT)']  ,axis=1, inplace=True, errors = 'ignore')\n",
    "aq_filtered.drop(['NOx(GT)'] ,axis=1, inplace=True, errors = 'ignore')\n",
    "aq_filtered.drop(['C6H6(GT)'],axis=1, inplace=True, errors = 'ignore')\n",
    "aq_filtered.drop(['NO2(GT)'] ,axis=1, inplace=True, errors = 'ignore')\n",
    "aq_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42993558",
   "metadata": {},
   "source": [
    "Ok, I'll generate one final report before continuing to check the state of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b5c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_filtered.reset_index(drop=True, inplace=True) #Drop index prior to generating report\n",
    "report = ProfileReport(aq_filtered)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c0e16",
   "metadata": {},
   "source": [
    "Nice! The variables are now nice and normal, I've dealt with outliers and there are some interesting relationships between variables that I can explore as part of the model I'll build.\n",
    "\n",
    "Some questions that may be interesting to look into are:\n",
    "\n",
    "* Can we predict the level of a pollutant based on the concentrations of other pollutants?\n",
    "* At what times/days do pollutant levels maximize/minimize?\n",
    "* Do all pollutants increase together or do some of them peak/valley at different points?\n",
    "* Is there any relationship/trend we can infer/deduce with regards to the humidity/temperature from pollutant concentrations?\n",
    "\n",
    "I think I'll start by looking into the behavior of each of these concentrations as a function of time\n",
    "\n",
    "## Time Series on Air Quality Data\n",
    "Since one of the questions I'm interested is seeing if there are certain days/months that have worse pollution than others, I'll add a 'day' column to my dataframe before anything else.\n",
    "\n",
    "Let's start visualizing stuff! I'm going to start by checking the CO concentrations on a monthly, daily and hourly basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_df_list = []\n",
    "day_df_list   = []\n",
    "hour_df_list  = []\n",
    "\n",
    "months = ['January','February','March', 'April', 'May','June', \n",
    "          'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "for month in months:\n",
    "    temp_df = aq_filtered.loc[(aq_filtered['Month'] == month)]\n",
    "    month_df_list.append(temp_df)\n",
    "\n",
    "for day in days:\n",
    "    temp_df = aq_filtered.loc[(aq_filtered['Weekday'] == day)]\n",
    "    day_df_list.append(temp_df)\n",
    "\n",
    "for hour in range(24):\n",
    "    temp_df = aq_filtered.loc[(aq_filtered['Hour'] == hour)]\n",
    "    hour_df_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1300f3a",
   "metadata": {},
   "source": [
    "Sweet! Now I can start visualizing the data attributes at different temporal specificities. I'll make a  function that will allow me to visualize the different pollutant concentrations at different months, days, and hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4770585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_time_plotter(df_list, time_unit, y_col):\n",
    "    \n",
    "    months = ['January','February','March', 'April', 'May','June', \n",
    "              'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    \n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    \n",
    "    if time_unit == 'M':\n",
    "        nRows = 3\n",
    "        nCols = 4\n",
    "        n_iter = len(months)\n",
    "    elif time_unit == 'D':\n",
    "        nRows = 2\n",
    "        nCols = 4\n",
    "        n_iter = len(days)\n",
    "    elif time_unit == 'H':\n",
    "        nRows = 4\n",
    "        nCols = 6\n",
    "        n_iter = 24\n",
    "    else:\n",
    "        print('time_unit must be a string equal to M,D, or H')\n",
    "        return 0\n",
    "        \n",
    "    fig, axs = plt.subplots(nrows=nRows, ncols=nCols, figsize = (40,30))\n",
    "    axs = axs.ravel()\n",
    "    for i in range(n_iter):\n",
    "        data = df_list[i]\n",
    "        ax = axs[i]\n",
    "        data.plot(kind ='scatter', x = 'DateTime', y= y_col , ax = ax, fontsize = 24)\n",
    "        ax.set_ylabel('Pollutant Concentration',fontsize=30)\n",
    "        ax.set_xlabel('')\n",
    "        if time_unit == 'M':\n",
    "            ax.set_title(y_col + ' ' + months[i],  size=40) # Title\n",
    "        elif time_unit == 'D':\n",
    "            ax.set_title(y_col + ' ' + days[i],  size=40) # Title\n",
    "        else:\n",
    "             ax.set_title(y_col + ' ' + str(i),  size=40) # Title\n",
    "        ax.tick_params(labelrotation=60)\n",
    "\n",
    "        #plt.xlim([datetime.date(2004, 3, 10), datetime.date(2004, 3, 30)])\n",
    "    # set the spacing between subplots\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.5)\n",
    "    plt.show() # Depending on whether you use IPython or interactive mode, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5fb4f",
   "metadata": {},
   "source": [
    "Let's see how the the CO levels change every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ed850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_plotter(month_df_list,'M','PT08.S1(CO)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ba39d",
   "metadata": {},
   "source": [
    "Hmmm, the CO levels are highest in November and December. The sensors had a lot of missing/bad readings on March and April. Let's see how the concentrations change for every day of the week,|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_plotter(day_df_list,'D','PT08.S1(CO)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76847d97",
   "metadata": {},
   "source": [
    "Hmm, the CO levels are lowest on Saturday and Sunday it appears. Let's see it as a function of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c113ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_plotter(hour_df_list,'H','PT08.S1(CO)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11887960",
   "metadata": {},
   "source": [
    "It looks like readings are quite low between 4-6 AM. The CO levels rise starting at 1PM and peak at around 6-8pm. Maybe a bar plot will make some of these relationships more apparent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = 'Month', y = 'PT08.S1(CO)', data = aq_filtered)\n",
    "plt.title('CO Values Per Month')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x = 'Weekday', y = 'PT08.S1(CO)', data = aq_filtered)\n",
    "plt.title('CO Values Per Day of the Week')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x = 'Hour', y = 'PT08.S1(CO)', data = aq_filtered)\n",
    "plt.title('CO Values Per Hour')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b90f8",
   "metadata": {},
   "source": [
    "This is a lot clearer. \n",
    "\n",
    "From these plots we can see the following:\n",
    "\n",
    "* October has the highest CO readings while August had the lowest readings. \n",
    "* CO levels trend downward from October to August. They start to rise between August to October.\n",
    "* CO levels are lowest on Sundays and highest on Friday. \n",
    "* CO levels are lowest between 4-5 AM and highest at 8 AM and 7 PM. This makes sense since these are rush hour times.\n",
    "\n",
    "Let me generate a pairplot to get a better grasp of all these relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_final = aq_filtered.iloc[:, 5:]\n",
    "aq_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17162f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "sns.pairplot(aq_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d988c",
   "metadata": {},
   "source": [
    "Beautiful! Clearly the concentrations of pollutants are dependent on one another. To what extent though?\n",
    "\n",
    "Strangely, the NO2 sensor shows some monotonicity with Temperature and Ambient Humidity. This is something that the other pollutants don't exhibit with those two variables. It'll be interesting to explore that. \n",
    "\n",
    "I've answered the time dependence questions I had regarding pollutants and time. \n",
    "\n",
    "Now I want to determine whether I can use this data to accurately predict pollutant concentration.\n",
    "\n",
    "I'll be building a model for each of the following pollutants/sensors:\n",
    "\n",
    "* PT08.S1(CO)\n",
    "* PT08.S2(NMHC)\n",
    "* PT08.S3(NOx)\n",
    "* PT08.S4(NO2)\n",
    "* PT08.S5(O3)\n",
    "\n",
    "## Splitting Data and Building Models\n",
    "I have different variables I want to predict. Furthermore, I'm interesting in testing the performance of a variety of different regressors to generate a model. In order to make things a bit cleaner code-wise, I'll write a function that will allow me to split the data for each dependent variable and test each of the 10 regressors for a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_assess(X_train, X_test, y_train, y_test, model, title = \"Default\"):\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred  = model.predict(X_test)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    results = pd.DataFrame([title,train_mse, train_r2, test_mse, test_r2]).transpose()\n",
    "    results.columns = ['Method','Training MSE','Training R2','Test MSE','Test R2']\n",
    "    return y_train_pred,y_test_pred, results\n",
    "\n",
    "def multi_model_assess(df, models, y_predict):\n",
    "    all_model_results = [] #This will contain all model results for each dependent variable \n",
    "    all_X_test  = []\n",
    "    all_X_train = []\n",
    "    all_y_test_p  = []\n",
    "    all_y_train_p = []\n",
    "    all_y_train = []\n",
    "    #First loop will define dependent/independent variables and split data into test/training sets\n",
    "    n_vars = len(y_predict)\n",
    "    pbar = tqdm(range(n_vars), desc=\"Variable Processed\", position = 0, leave = True)#Add progress bar \n",
    "    \n",
    "    for dependent in y_predict:\n",
    "        model_results = [] #Array with dataframes for a given dependent variable\n",
    "        #Designate independent and dependent variables\n",
    "        x  = df.drop([dependent], axis = 1)\n",
    "        y  = df[dependent]\n",
    "        #Split data into test and training sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "        \n",
    "        #Populate the array of observed values for the dependent variable\n",
    "        all_y_train.append(y_train)\n",
    "        \n",
    "        #Process each of the desired models\n",
    "        for model, model_name in models:\n",
    "            y_train_pred,y_test_pred, results = model_assess(X_train, X_test, y_train, y_test,\n",
    "                                                             model, title = model_name)\n",
    "            \n",
    "            model_results.append(results)\n",
    "            all_X_test.append(X_test)\n",
    "            all_X_train.append(X_train)\n",
    "            all_y_test_p.append(y_test_pred)\n",
    "            all_y_train_p.append(y_train_pred)\n",
    "                    \n",
    "        all_model_results.append(model_results)\n",
    "        pbar.update(1)\n",
    "        pbar.refresh()\n",
    "        \n",
    "    pbar.close()   \n",
    "    return all_model_results, all_X_test, all_X_train, all_y_test_p, all_y_train_p, all_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ee754",
   "metadata": {},
   "source": [
    "Great! Now let's initiate each of the models, run them, and see how they performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate Different Regressors for ML model\n",
    "lr = LinearRegression() \n",
    "hr = HuberRegressor(epsilon=1.15, max_iter=1000)\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "gb = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "gp = GaussianProcessRegressor(kernel = DotProduct() + WhiteKernel(), random_state=42)\n",
    "kn = KNeighborsRegressor()\n",
    "ab = AdaBoostRegressor()\n",
    "sv = SVR()\n",
    "dt = DecisionTreeRegressor(max_features = 'auto', max_depth=3, random_state=42)\n",
    "nn = MLPRegressor(hidden_layer_sizes = 500, solver='adam', learning_rate_init = 1e-2,max_iter=500)\n",
    "\n",
    "models =  [(lr,'Linear Regression'), \n",
    "           (hr,'Huber Regression'), \n",
    "           (rf,'Random Forest'), \n",
    "           (gb,'Gradient Boosting'),\n",
    "           (gp,'Gaussian Process'), \n",
    "           (kn,'K-Neighbors'), \n",
    "           (ab,'Ada Boost'), \n",
    "           (sv,'SVR'), \n",
    "           (dt,'Decision Tree'), \n",
    "           (nn,'MLP')]\n",
    "\n",
    "y_predict  = ['PT08.S1(CO)','PT08.S2(NMHC)','PT08.S3(NOx)','PT08.S4(NO2)','PT08.S5(O3)']\n",
    "\n",
    "all_model_results, _, _, all_y_test_p, all_y_train_p, all_y_train = multi_model_assess(aq_final,models, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11755e",
   "metadata": {},
   "source": [
    "Nice! We were able to generate a total of 10 different regressor models for each of the 5 target variables in just under 7 minutes.  Let's see how each of them performed.\n",
    "\n",
    "## Initial Assesment of Performance for Regression Models\n",
    "The initial results I have obtained here for each regressor model are based on little to no modifications from the default values for each regressor. Here I'll determine which models to refine further based using metrics like R2 and Mean Squared Error (MSE) I'll use the results of this assement to carry out a parameter grid-search/exploration to optimize the model.  \n",
    "\n",
    "### Models for CO Concentration Prediction\n",
    "The results for each of the 10 models built for the prediction of CO concentration are shown below. From "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_results = pd.concat(all_model_results[0], \n",
    "                        ignore_index=True).sort_values('Test R2',axis = 0, ascending = False)\n",
    "score_df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ccc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_results_test = pd.concat(all_model_results[0], ignore_index=True)\n",
    "score_results_test['Test R2'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363413a",
   "metadata": {},
   "source": [
    "The top 3 regressor models for the prediction of CO concentration based on the R2 value of the test set resulted from:\n",
    "\n",
    "1. Gradient Boosting\n",
    "2. K-Neighbors\n",
    "3. Linear Regression\n",
    "\n",
    "I'll plot them to see how they compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1067da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define column with model to predict\n",
    "y_predict  = ['PT08.S1(CO)','PT08.S2(NMHC)','PT08.S3(NOx)','PT08.S4(NO2)','PT08.S5(O3)']\n",
    "\n",
    "#Model names for plot titles\n",
    "models =  [(lr,'Linear Regression'), \n",
    "           (hr,'Huber Regression'), \n",
    "           (rf,'Random Forest'), \n",
    "           (gb,'Gradient Boosting'),\n",
    "           (gp,'Gaussian Process'), \n",
    "           (kn,'K Neighbors'), \n",
    "           (ab,'Ada Boost'), \n",
    "           (sv,'SVR'), \n",
    "           (dt,'Decision Tree'), \n",
    "           (nn,'MLP')]\n",
    "\n",
    "#Make labels\n",
    "def make_labels(models):\n",
    "    names = []\n",
    "    for i in range(len(models)):\n",
    "        if len(models[i][1].split()) < 2:\n",
    "            names.append(models[i][1])\n",
    "        else:\n",
    "            names.append(''.join([s[0] for s in models[i][1].split()]))\n",
    "    return names\n",
    "labelList = make_labels(models)\n",
    "\n",
    "#Specify color map to color different plots\n",
    "cmap = plt.cm.get_cmap('plasma')\n",
    "slicedCM = cmap(np.linspace(0, 1, len(models))) \n",
    "\n",
    "#Visualize results of linear regression\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "nRows = 5 # \n",
    "nCols = 2 #\n",
    "\n",
    "def plot_ML_model(whichVar):\n",
    "    fig, axs = plt.subplots(nrows=nRows, ncols=nCols, figsize = (15,30))\n",
    "    axs = axs.ravel()\n",
    "    df    = pd.concat(all_model_results[whichVar], ignore_index=True)\n",
    "    for k in range(10):\n",
    "        color = slicedCM[k]\n",
    "        yPred = all_y_train_p[k + whichVar*len(models)]\n",
    "        yMeas = all_y_train[whichVar]\n",
    "        label = labelList[k]\n",
    "        ax    = axs[k]\n",
    "        #Make scatter plot of train set and regressor model\n",
    "        ax.scatter(x = yMeas, y = yPred, color=color, alpha=0.5)\n",
    "\n",
    "        #Fit a first order polynomial (i.e. a straight line) to the regressor model \n",
    "        z = np.polyfit(yMeas, yPred, 1)\n",
    "        p = np.poly1d(z)\n",
    "\n",
    "        #Add labels and colors and stuff\n",
    "        val = df['Test R2'][k] #Get the r2 value from the model results dataframe\n",
    "        val = \"{:.2f}\".format(val)\n",
    "        ax.plot(yMeas,p(yMeas),\"#b20cd7\", label=label+\"\\nr\\u00b2\".format(2) + \" = \" + str(val))\n",
    "        ax.title.set_text(models[k][1]) \n",
    "        ax.set(xlabel='Train Concentration', ylabel='Predicted Concentration')\n",
    "        ax.label_outer()\n",
    "        ax.legend(loc=\"upper left\")\n",
    "        ax.grid(color = 'black', linestyle = '--', linewidth = 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737db99",
   "metadata": {},
   "source": [
    "Great! Now plotting is nice and automated. Let's see the plots for the CO fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ML_model(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ea5f0",
   "metadata": {},
   "source": [
    "### Models for NMHC Concentration Prediction\n",
    "The results for each of the 10 models built for the prediction of NMHC concentration are shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12088296",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_results = pd.concat(all_model_results[1], \n",
    "                        ignore_index=True).sort_values('Test R2',axis = 0, ascending = False)\n",
    "score_df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ea465",
   "metadata": {},
   "source": [
    "The top 3 regressor models for the prediction of CO concentration based on the R2 value of the test set resulted from:\n",
    "\n",
    "1. Gradient Boosting\n",
    "2. Huber Regression\n",
    "3. Linear Regression\n",
    "\n",
    "I'll plot them to see how they compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ML_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b5b25a",
   "metadata": {},
   "source": [
    "### Models for NOx Concentration Prediction\n",
    "The results for each of the 10 models built for the prediction of NOx concentration are shown below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc76437",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_results = pd.concat(all_model_results[2], \n",
    "                        ignore_index=True).sort_values('Test R2',axis = 0, ascending = False)\n",
    "score_df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a6b1b",
   "metadata": {},
   "source": [
    "The top 3 regressor models for the prediction of NOx concentration based on the R2 value of the test set resulted from:\n",
    "\n",
    "1. Gradient Boosting\n",
    "2. K-Neighbors\n",
    "3. Linear Regression\n",
    "\n",
    "I'll plot them to see how they compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ML_model(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa640ab",
   "metadata": {},
   "source": [
    "### Models for NO2 Concentration Prediction\n",
    "The results for each of the 10 models built for the prediction of NO2 concentration are shown below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c82919",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_results = pd.concat(all_model_results[3], \n",
    "                        ignore_index=True).sort_values('Test R2',axis = 0, ascending = False)\n",
    "score_df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ae6d9",
   "metadata": {},
   "source": [
    "The top 3 regressor models for the prediction of NO2 concentration based on the R2 value of the test set resulted from:\n",
    "\n",
    "1. Gradient Boosting\n",
    "2. K-Neighbors\n",
    "3. Linear Regression\n",
    "\n",
    "I'll plot them to see how they compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0114efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ML_model(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac872c0",
   "metadata": {},
   "source": [
    "\n",
    "### Models for O3 Concentration Prediction\n",
    "The results for each of the 10 models built for the prediction of NOx concentration are shown below. From  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617a4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_results = pd.concat(all_model_results[4], \n",
    "                        ignore_index=True).sort_values('Test R2',axis = 0, ascending = False)\n",
    "score_df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e89d8",
   "metadata": {},
   "source": [
    "The top 3 regressor models for the prediction of NO2 concentration based on the R2 value of the test set resulted from:\n",
    "\n",
    "1. Gradient Boosting\n",
    "2. MLP\n",
    "3. K-Neighbors\n",
    "\n",
    "The MLP did quite well for this particular dataset. I wonder why?\n",
    "\n",
    "I'll plot them to see how they compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ML_model(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf1545",
   "metadata": {},
   "source": [
    "Overall GradientBoost generated the best predictor model for each of these pollutants. Therefore, I'll devote the remainder of this project to optimizing it via a parameter grid-search \n",
    "\n",
    "## Parameter Space Exploration for Gradient Boosting (GB) Model\n",
    "Since the GB model had the best prediction accuracy for all compounds, I will now optimize the parameters to see how much more improvement I can make on it. \n",
    "\n",
    "Let me start by getting the current parameters of the GB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_params = gb.get_params()\n",
    "ini_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570fc3d6",
   "metadata": {},
   "source": [
    "Now I'll set up the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict  = ['PT08.S1(CO)','PT08.S2(NMHC)','PT08.S3(NOx)','PT08.S4(NO2)','PT08.S5(O3)']\n",
    "\n",
    "compound = y_predict[0]\n",
    "#Designate independent and dependent variables\n",
    "x  = aq_final.drop([compound], axis = 1)\n",
    "y  = aq_final[compound]\n",
    "#Split data into test and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "        \n",
    "#Initiate Different Regressors for ML model\n",
    "gb = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "#Setup values for parameters\n",
    "grid_values = {\n",
    "               'learning_rate':[0.05,0.1, 0.15],\n",
    "               'n_estimators':[150, 200, 225],\n",
    "               'max_depth': [5,6,7]\n",
    "              }\n",
    "\n",
    "grid_clf_acc = GridSearchCV(gb, param_grid = grid_values, scoring = 'r2', verbose=1)\n",
    "grid_clf_acc.fit(X_train, y_train)\n",
    "\n",
    "#Predict values based on new parameters\n",
    "y_train_pred = grid_clf_acc.predict(X_train)\n",
    "y_test_pred  = grid_clf_acc.predict(X_test)\n",
    "\n",
    "# New Model Evaluation metrics \n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print('r2 Score : ' + str(r2_score(y_test,y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6160a2da",
   "metadata": {},
   "source": [
    "Awesome! It's finished.\n",
    "\n",
    "## Optimized model\n",
    "Let's start seeing how the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b05b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dataframe for grid search results\n",
    "co_model = pd.DataFrame(['Gradient Boosting',train_mse, train_r2, test_mse, test_r2]).transpose()\n",
    "co_model.columns = ['Method','Training MSE','Training R2','Test MSE','Test R2']\n",
    "co_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d526bb",
   "metadata": {},
   "source": [
    "We have an R2 score of 0.91 which is not bad. Let's see which parameter combination resulted in the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2395c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_params = grid_clf_acc.best_params_\n",
    "fin_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecfe2f",
   "metadata": {},
   "source": [
    "Cool! Now I'll set the final GB model to have the values of the grid-search optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd43c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_GB_model = GradientBoostingRegressor(**grid_clf_acc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48c76d",
   "metadata": {},
   "source": [
    "And now I'll save the optimized model into a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model into pickle file\n",
    "with open('air_quality.pkl','wb') as f:\n",
    "    pickle.dump(optimized_GB_model,f)\n",
    "    \n",
    "# load model from pickle file\n",
    "with open('air_quality.pkl', 'rb') as f:\n",
    "    gb_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a1ceb",
   "metadata": {},
   "source": [
    "## Using GB Model to Predict CO Concentration\n",
    "Let's see how well it predicts stuff! I'll start by making predictions based on the test set. I'll see how well it performs on the entire dataset momentarily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_GB_model.fit(X_train, y_train)\n",
    "co_gb_predictions = optimized_GB_model.predict(X_test)\n",
    "print('Regression Model: R²={:.2f}'.format(r2_score(y_test, co_gb_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed9ebc1",
   "metadata": {},
   "source": [
    "Okay, we have R2  of 0.91. Let's compare the predictions with the observed values visualize how well the model replicates the actual concentrations of CO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff684fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame()\n",
    "final['Measured CO']   = y_test\n",
    "final['Predicted CO']  = co_gb_predictions\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.histplot(data=final)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5c2d7",
   "metadata": {},
   "source": [
    "Nice! The model follows the data quite well! Let's look at the distribution of the residuals. If our model is good (i.e., random error is normally distributed) then the residuals will distribute themselves in the shape of a symmetric Gaussian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "res = pd.DataFrame()\n",
    "res['residuals'] = final['Measured CO'] - final['Predicted CO']\n",
    "sns.histplot(data=res, kde = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b5b1c",
   "metadata": {},
   "source": [
    "Lovely! The errors in the model are normally distributed. \n",
    "\n",
    "Let's check for heteroskedasticity (i.e., error has non-constant variance) by plotting the residuals vs the predicted values. To avoid heteroskedasticity, the errors should not have any particular shape (they should be randomly oscillating around the mean or the zero line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.scatterplot(x= final['Predicted CO'], y=res['residuals'])\n",
    "plt.axhline(y=0.0, color='r', linestyle='-', linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703d122",
   "metadata": {},
   "source": [
    "Nice! The errors do not show heteroskedasticity. See how they oscillate around the zero line (red line). This suggests that the variances of the errors are equal.\n",
    "\n",
    "Next let's see the normality Q-Q plot. This plot is used to determine the normal distribution of errors. If the data is perfectly normally distributed, then all the points will be contained in a straight line. Deviations from this behavior suggest non-linearity is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aba58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,4))\n",
    "stats.probplot(res['residuals'], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43f13c7",
   "metadata": {},
   "source": [
    "Hmmm, interesting, we have some pretty apparent deviations from linearity at the upper ends of the resdiduals. However, the data is linear for most of the range. Maybe we could have been more stringent with our removal of outliers earlier?\n",
    "\n",
    "Let me see how well it does on the entire dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a5759",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict  = ['PT08.S1(CO)','PT08.S2(NMHC)','PT08.S3(NOx)','PT08.S4(NO2)','PT08.S5(O3)']\n",
    "\n",
    "compound = y_predict[0]\n",
    "\n",
    "#Designate independent and dependent variables\n",
    "x  = aq_final.drop([compound], axis = 1)\n",
    "y  = aq_final[compound]\n",
    "\n",
    "#Predict concentration over entire dataset\n",
    "optimized_GB_model.fit(x, y)\n",
    "co_gb_pred_full = optimized_GB_model.predict(x)\n",
    "print('Regression Model: R²={:.2f}'.format(r2_score(y, co_gb_pred_full)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c04bcd",
   "metadata": {},
   "source": [
    "Very nice! R2 is 0.98 suggesting high correlation between the predicted CO concentrations from the model and the observed CO concentrations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb18d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_final = pd.DataFrame()\n",
    "full_final['Measured CO']  = y\n",
    "full_final['Predicted CO'] = co_gb_pred_full\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.histplot(data=full_final)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8e312",
   "metadata": {},
   "source": [
    "Sweet! There's not too much difference between the predicted CO concentrations and the actual ones! Let's look at the residual distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2cdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "res_final = pd.DataFrame()\n",
    "res_final['residuals'] = full_final['Measured CO'] - full_final['Predicted CO']\n",
    "sns.histplot(data=res_final, kde = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789fbb1d",
   "metadata": {},
   "source": [
    "Lovely! The distribution looks Gaussian.\n",
    "\n",
    "Let's check for heteroskedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.scatterplot(x= full_final['Predicted CO'], y=res_final['residuals'])\n",
    "plt.axhline(y=0.0, color='r', linestyle='-', linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e407f4e",
   "metadata": {},
   "source": [
    "No heteroskedasticity here. Let's look at the Q-Q plot now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd502abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,4))\n",
    "stats.probplot(res_final['residuals'], dist=\"norm\", plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710fc97",
   "metadata": {},
   "source": [
    "Similar situation as when we used the test set for our predictions. Data is linear for most of the range. However, there is deviation from linearity at the both quantile ends suggesting that there is either further refinement that could be done in terms of outliers.\n",
    "\n",
    "## Final Comparison\n",
    "The plot below shows the observed vs predicted CO concentrations over the course of 1 year. The solid lines are the CO concentrations measured by the sensors while the dots are the predicted CO concentrations using the Gradient Boosting Regressor. The color scale over the x-axis is used to show the separation in months. As can be seen, the generated model and the observed values have excellent agreement with one another over the course of the entire year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "maxRow = 5250\n",
    "sns.scatterplot(x = aq_filtered['DateTime'][:maxRow], y = full_final['Predicted CO'][:maxRow],\n",
    "             hue = aq_filtered['Month'], palette = 'twilight_shifted', legend = False,\n",
    "             s=300)\n",
    "sns.lineplot(x = aq_filtered['DateTime'][:maxRow], y = full_final['Measured CO'][:maxRow],\n",
    "             hue = aq_filtered['Month'] , palette = 'twilight_shifted', \n",
    "             legend = 'auto')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0 ,fontsize = 30)\n",
    "plt.title('Observed vs Predicted CO concentrations', fontsize = 50)\n",
    "plt.ylabel('CO Concentration', fontsize = 50)\n",
    "plt.xlabel('Time[yy/mm]', fontsize = 50)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb2ed0",
   "metadata": {},
   "source": [
    "## Conclusions and Final Thoughts\n",
    "Models were built to predict the concentrations of 5 different pollutants (Carbon Monoxide, Nitrous Dioxide, Nitrous Oxide, Non-metal Hydrocarbons, and O3) measured over the course of 1 year using 10 different regressors. The Gradient Boosting (GB) regressor had the best performance across the board for all compounds. This model was optimized for the prediction of CO via a grid-search exploration and an R2 score of 0.98 was obtained. The GB model was used to predict the CO concentrations over the entire dataset and showed excellent agreement with the observed values. The performance of the models generated for the other pollutants is expected to be comparable to the one observed for CO.\n",
    "\n",
    "A few things that were observed from this data are:\n",
    "\n",
    "* October has the highest CO readings while August had the lowest readings. \n",
    "* CO levels trend downward from October to August. They start to rise between August to October.\n",
    "* CO levels are lowest on Sundays and highest on Friday. \n",
    "* CO levels are lowest between 4-5 AM and highest at 8 AM and 7 PM. \n",
    "* The NO levels decrease when the other pollutant concentrations increase. NO levels are also correlated to ambient humidity. \n",
    "\n",
    "The final point was interesting to me. Why doesn't NO concentration increase alongside the other pollutants? \n",
    "\n",
    "My initial hypothesis is that NO is reacting with water and/or with some of the other polluting compounds. NO is a free radical compound (meaning that it is extremely unstable and highly reactive) formed by the reaction of nitrogen gas, N2, and oxygen gas, O2. This means that it is very likely that there is a reaction taking place that would produce something other than NO hence decreasing its concentration. A potential reaction for this would be:\n",
    "\n",
    "1. $ 2NO(g) + O_{2}(g) -> 2NO_{2}(g) $\n",
    "\n",
    "2. $ 2NO_{2}(g) + H_{2}O(l) -> HNO_{3}(aq) + HNO_{2}(aq)$\n",
    "\n",
    "In the first reaction, Nitrogen Monoxide reacts with Oxygen gas to form Nitrogen Dioxide. Then,  Nitrogen Dioxide can react with water to form a mixture of Nitrous Acid and Nitric Acid. Nitric Acid is one of the components of acid rain.\n",
    "\n",
    "This was a fun project to work on where I was able to do clean up time series data, build regressor models, and optimize them to get a accurate predictions of pollutant concentrations. \n",
    "\n",
    "I hope you find it interesting and/or useful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
